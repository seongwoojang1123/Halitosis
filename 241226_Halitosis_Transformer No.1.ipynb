{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1hBtcueYFvuOgr8TVzOWkpSRAcIoCFciV","authorship_tag":"ABX9TyOPmj8U6Gn0eihNW0NsW+6v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtYzukWwEheR","executionInfo":{"status":"ok","timestamp":1735188503208,"user_tz":-540,"elapsed":2825,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"aebc7e0e-5719-4fb1-cc1f-f0b64a574657"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Features Shape: (821, 48)\n","Target Variable Shape: (821,)\n","Input Features Sample:\n","   Sex   Age  Elderly  Super_Elderly  Xerostomia_subjective   UFR  SFR   pH  \\\n","0    0  13.5        0              0                      0  0.75  2.1  7.8   \n","1    0  12.0        0              0                      0  0.50  1.3  7.4   \n","2    0  15.2        0              0                      0  0.40  1.0  7.2   \n","3    0  10.0        0              0                      0  0.40  0.6  7.0   \n","4    0  16.0        0              0                      0  0.30  1.0  7.2   \n","\n","   BufferCapacity  VAS  ...  M_Hyperlipidemia  M_CV  M_TD  M_GI  \\\n","0            12.0    6  ...                 0     0     0     0   \n","1            12.0    5  ...                 0     0     0     0   \n","2            12.0    0  ...                 0     0     0     0   \n","3            12.0    2  ...                 0     0     0     0   \n","4            12.0    2  ...                 0     0     0     0   \n","\n","   M_UrinaryDiseases  M_Arthritis  M_Rheumatism  M_Anxiolytic  \\\n","0                  0            0             0             0   \n","1                  0            0             0             0   \n","2                  0            0             0             0   \n","3                  0            0             0             0   \n","4                  0            0             0             0   \n","\n","   M_SleepingPills  M_Aspirin  \n","0                0          0  \n","1                0          0  \n","2                0          0  \n","3                0          0  \n","4                0          0  \n","\n","[5 rows x 48 columns]\n","\n","Target Variable Sample:\n","0    0\n","1    1\n","2    1\n","3    1\n","4    0\n","Name: Halitosis_subjective, dtype: int64\n"]}],"source":["import pandas as pd\n","\n","# 1. 데이터 로드\n","file_path = '/content/drive/MyDrive/Halitosis/241121_Halitosis_data preprocessing.xlsx'\n","data = pd.read_excel(file_path)\n","\n","# 2. 입력 Feature와 타겟 변수 설정\n","input_features = [\n","    'Sex', 'Age', 'Elderly', 'Super_Elderly', 'Xerostomia_subjective',\n","    'UFR', 'SFR', 'pH', 'BufferCapacity', 'VAS', 'StickySaliva', 'Oralhygiene',\n","    'Calculus', 'O_Stomatitis', 'O_RAU', 'O_Candidiasis', 'O_Periodontitis',\n","    'O_LichenPlanus', 'O_Sialodochitis', 'O_Glossodynia', 'O_BMS',\n","    'S_Hypertension', 'S_DM', 'S_Hyperlipidemia', 'S_RenalDiseases',\n","    'S_LiverDiseases', 'S_Rheumatism', 'S_Osteoporosis', 'S_CVD', 'S_TD',\n","    'S_MentalDisorders', 'S_UrinaryDiseases', 'S_Asthma', 'S_CancerOp',\n","    'NumberofSystmicDiseases', 'M_Hypertension', 'M_DM', 'M_Osteoporosis',\n","    'M_Hyperlipidemia', 'M_CV', 'M_TD', 'M_GI', 'M_UrinaryDiseases',\n","    'M_Arthritis', 'M_Rheumatism', 'M_Anxiolytic', 'M_SleepingPills',\n","    'M_Aspirin'\n","]\n","target_feature = 'Halitosis_subjective'\n","\n","# Feature와 Target 분리\n","X = data[input_features]\n","y = data[target_feature]\n","\n","# 데이터 크기 확인\n","print(f\"Input Features Shape: {X.shape}\")\n","print(f\"Target Variable Shape: {y.shape}\")\n","\n","# 데이터 예시 출력\n","print(\"Input Features Sample:\")\n","print(X.head())\n","print(\"\\nTarget Variable Sample:\")\n","print(y.head())"]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","# 1. 클래스 불균형 해결 (SMOTE)\n","smote = SMOTE(random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X, y)\n","\n","# 클래스 분포 확인\n","print(\"Original class distribution:\")\n","print(y.value_counts())\n","print(\"\\nResampled class distribution:\")\n","print(pd.Series(y_resampled).value_counts())\n","\n","# 2. 데이터 정규화 (StandardScaler)\n","scaler = StandardScaler()\n","X_resampled_normalized = scaler.fit_transform(X_resampled)\n","\n","# 데이터 형태 확인\n","print(\"\\nResampled and Normalized Data Shape:\")\n","print(f\"Features Shape: {X_resampled_normalized.shape}\")\n","print(f\"Target Shape: {y_resampled.shape}\")\n","\n","# 정규화된 데이터 샘플 출력\n","print(\"\\nNormalized Data Sample:\")\n","print(X_resampled_normalized[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxHGoQlQGCc7","executionInfo":{"status":"ok","timestamp":1735188513786,"user_tz":-540,"elapsed":3058,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"357915b3-0ca6-4f57-a69a-3822c69bfb2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original class distribution:\n","Halitosis_subjective\n","0    687\n","1    134\n","Name: count, dtype: int64\n","\n","Resampled class distribution:\n","Halitosis_subjective\n","0    687\n","1    687\n","Name: count, dtype: int64\n","\n","Resampled and Normalized Data Shape:\n","Features Shape: (1374, 48)\n","Target Shape: (1374,)\n","\n","Normalized Data Sample:\n","[[-1.22029932 -2.41470021 -0.80220987 -0.76452233 -0.48997163  1.65034332\n","   1.40329025  1.71775386  0.66942231  0.17661126 -0.5127062   0.2992409\n","   0.46004628  2.15201012 -0.17974019 -0.33279396 -0.13888065 -0.1731618\n","  -0.06622662 -0.11841512 -0.56558353 -0.60145183 -0.30651108 -0.14940358\n","  -0.10854507 -0.1468378  -0.12756249 -0.31913074 -0.21554232 -0.19832887\n","  -0.15441829 -0.16635473 -0.1215362  -0.15687097 -0.7617197  -0.27407548\n","  -0.15687097 -0.13047769 -0.26158983 -0.16402997 -0.14423029 -0.1468378\n","  -0.13888065 -0.11521431 -0.05403432 -0.1731618  -0.17091952 -0.14157875]\n"," [-1.22029932 -2.49781521 -0.80220987 -0.76452233 -0.48997163  0.58433793\n","   0.10170979  1.06742671  0.66942231 -0.15717673 -0.5127062   0.2992409\n","   0.46004628 -0.46468183 -0.17974019 -0.33279396 -0.13888065 -0.1731618\n","  -0.06622662 -0.11841512 -0.56558353 -0.60145183 -0.30651108 -0.14940358\n","  -0.10854507 -0.1468378  -0.12756249 -0.31913074 -0.21554232 -0.19832887\n","  -0.15441829 -0.16635473 -0.1215362  -0.15687097 -0.7617197  -0.27407548\n","  -0.15687097 -0.13047769 -0.26158983 -0.16402997 -0.14423029 -0.1468378\n","  -0.13888065 -0.11521431 -0.05403432 -0.1731618  -0.17091952 -0.14157875]\n"," [-1.22029932 -2.32050321 -0.80220987 -0.76452233 -0.48997163  0.15793577\n","  -0.38638289  0.74226313  0.66942231 -1.82611666 -0.5127062   0.2992409\n","   0.46004628 -0.46468183 -0.17974019 -0.33279396 -0.13888065 -0.1731618\n","  -0.06622662 -0.11841512 -0.56558353 -0.60145183 -0.30651108 -0.14940358\n","  -0.10854507 -0.1468378  -0.12756249 -0.31913074 -0.21554232 -0.19832887\n","  -0.15441829 -0.16635473 -0.1215362  -0.15687097 -0.7617197  -0.27407548\n","  -0.15687097 -0.13047769 -0.26158983 -0.16402997 -0.14423029 -0.1468378\n","  -0.13888065 -0.11521431 -0.05403432 -0.1731618  -0.17091952 -0.14157875]\n"," [-1.22029932 -2.60863521 -0.80220987 -0.76452233 -0.48997163  0.15793577\n","  -1.03717312  0.41709956  0.66942231 -1.15854069 -0.5127062   0.2992409\n","  -1.46124123 -0.46468183 -0.17974019 -0.33279396 -0.13888065 -0.1731618\n","  -0.06622662 -0.11841512 -0.56558353 -0.60145183 -0.30651108 -0.14940358\n","  -0.10854507 -0.1468378  -0.12756249 -0.31913074 -0.21554232 -0.19832887\n","  -0.15441829 -0.16635473 -0.1215362  -0.15687097 -0.7617197  -0.27407548\n","  -0.15687097 -0.13047769 -0.26158983 -0.16402997 -0.14423029 -0.1468378\n","  -0.13888065 -0.11521431 -0.05403432 -0.1731618  -0.17091952 -0.14157875]\n"," [-1.22029932 -2.27617521 -0.80220987 -0.76452233 -0.48997163 -0.26846639\n","  -0.38638289  0.74226313  0.66942231 -1.15854069 -0.5127062  -1.59549179\n","  -1.46124123 -0.46468183 -0.17974019 -0.33279396 -0.13888065 -0.1731618\n","  -0.06622662 -0.11841512 -0.56558353 -0.60145183 -0.30651108 -0.14940358\n","  -0.10854507 -0.1468378  -0.12756249 -0.31913074 -0.21554232 -0.19832887\n","  -0.15441829 -0.16635473 -0.1215362  -0.15687097 -0.7617197  -0.27407548\n","  -0.15687097 -0.13047769 -0.26158983 -0.16402997 -0.14423029 -0.1468378\n","  -0.13888065 -0.11521431 -0.05403432 -0.1731618  -0.17091952 -0.14157875]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","\n","# 데이터 분리 (Train/Test Split)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# SMOTE 적용 (Train 데이터에만)\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","\n","# 클래스 분포 확인\n","print(\"Original class distribution in Train set:\")\n","print(y_train.value_counts())\n","print(\"\\nResampled class distribution in Train set:\")\n","print(pd.Series(y_train_resampled).value_counts())\n","\n","print(\"\\nOriginal class distribution in Test set:\")\n","print(y_test.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5B6GMZ_DHU3x","executionInfo":{"status":"ok","timestamp":1735188519420,"user_tz":-540,"elapsed":318,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"e4adc27d-57c3-4c14-ce7c-871638f425ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original class distribution in Train set:\n","Halitosis_subjective\n","0    549\n","1    107\n","Name: count, dtype: int64\n","\n","Resampled class distribution in Train set:\n","Halitosis_subjective\n","0    549\n","1    549\n","Name: count, dtype: int64\n","\n","Original class distribution in Test set:\n","Halitosis_subjective\n","0    138\n","1     27\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, input_dim, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, input_dim)\n","        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, input_dim, 2).float() * (-np.log(10000.0) / input_dim))\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)  # 짝수 인덱스에 Sine 적용\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)  # 홀수 인덱스에 Cosine 적용\n","        self.encoding = self.encoding.unsqueeze(0)  # Batch 차원 추가\n","\n","    def forward(self, x):\n","        # 입력 데이터에 Positional Encoding 추가\n","        seq_len = x.size(1)\n","        return x + self.encoding[:, :seq_len, :].to(x.device)\n","\n","# Transformer 모델 수정\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, num_heads, num_layers):\n","        super(TransformerModel, self).__init__()\n","        self.input_layer = nn.Linear(input_dim, 64)  # 입력 Feature 차원 -> 모델 차원 변환\n","        self.positional_encoding = PositionalEncoding(input_dim=64)  # Positional Encoding 추가\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=num_heads, batch_first=True, dropout=0.1)\n","        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)  # Transformer 인코더\n","        self.output_layer = nn.Linear(64, 1)  # 출력 레이어 (1차원: Binary Classification)\n","\n","    def forward(self, x):\n","        x = self.input_layer(x).unsqueeze(1)  # Linear 변환 후 차원 추가\n","        x = self.positional_encoding(x)  # Positional Encoding 추가\n","        x = self.transformer(x).squeeze(1)  # Transformer Encoder 통과\n","        x = self.output_layer(x)  # 출력 계산\n","        return torch.sigmoid(x)  # 확률 값 반환\n","\n","\n","# Positional Encoding 테스트\n","input_dim = 48  # 입력 Feature 개수\n","seq_len = 48    # Sequence 길이 (Feature 수와 동일)\n","\n","# 임의의 입력 데이터\n","dummy_input = torch.zeros(1, seq_len, input_dim)  # [Batch, Sequence Length, Feature Dimension]\n","\n","# Positional Encoding 적용\n","pos_encoder = PositionalEncoding(input_dim=input_dim)\n","encoded_input = pos_encoder(dummy_input)\n","\n","print(\"Original Input Shape:\", dummy_input.shape)\n","print(\"Encoded Input Shape:\", encoded_input.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdOkzNggTFyS","executionInfo":{"status":"ok","timestamp":1735188904132,"user_tz":-540,"elapsed":8,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"847916ab-3401-42a2-b4be-5cd4dd1103c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Input Shape: torch.Size([1, 48, 48])\n","Encoded Input Shape: torch.Size([1, 48, 48])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Custom Transformer 모델 수정\n","class CustomTransformerModel(nn.Module):\n","    def __init__(self, input_dim, num_heads, num_layers):\n","        super(CustomTransformerModel, self).__init__()\n","        self.input_layer = nn.Linear(input_dim, 64)  # 입력 차원을 임베딩 차원으로 변환\n","        self.positional_encoding = PositionalEncoding(input_dim=64)  # Positional Encoding 추가\n","        self.multihead_attn = nn.MultiheadAttention(embed_dim=64, num_heads=num_heads, batch_first=True)  # Multi-Head Attention\n","        self.encoder_layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=64, nhead=num_heads, batch_first=True, dropout=0.1)\n","            for _ in range(num_layers)\n","        ])\n","        self.feedforward = nn.Linear(64, 32)  # Feedforward Layer 추가\n","        self.output_layer = nn.Linear(32, 1)  # 최종 출력 레이어\n","        self.attention_weights = None  # Attention Weights 저장 변수\n","\n","    def forward(self, x):\n","        x = self.input_layer(x).unsqueeze(1)  # [Batch, Sequence Length, Feature Dimension]\n","        x = self.positional_encoding(x)\n","\n","        # Multi-Head Attention\n","        attn_output, attn_weights = self.multihead_attn(x, x, x)\n","        self.attention_weights = attn_weights  # Attention Weights 저장\n","\n","        # Transformer Encoder Layers\n","        for layer in self.encoder_layers:\n","            x = layer(x)\n","\n","        # Feedforward Layers\n","        x = self.feedforward(x[:, 0, :])  # 첫 번째 시퀀스 출력 사용\n","        x = self.output_layer(x)  # 최종 출력 계산\n","        return torch.sigmoid(x)  # 이진 분류를 위한 Sigmoid"],"metadata":{"id":"lidC3JZfVTEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# 모델 초기화\n","model = CustomTransformerModel(input_dim=X_train.shape[1], num_heads=4, num_layers=2)\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# 1. Tensor로 변환\n","X_train = torch.tensor(X_train.values, dtype=torch.float32)\n","X_test = torch.tensor(X_test.values, dtype=torch.float32)\n","y_train = torch.tensor(y_train.values, dtype=torch.float32)\n","y_test = torch.tensor(y_test.values, dtype=torch.float32)\n","\n","# 2. 학습 및 평가\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Forward Pass\n","    outputs = model(X_train).squeeze()\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","\n","    # 평가\n","    model.eval()\n","    with torch.no_grad():\n","        test_outputs = model(X_test).squeeze()\n","        predictions = (test_outputs >= 0.5).float()\n","        accuracy = (predictions == y_test).sum().item() / len(y_test)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWQmZnb5VVXD","executionInfo":{"status":"ok","timestamp":1735189364885,"user_tz":-540,"elapsed":1189,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"bcc2a64c-91bc-4335-f94b-0d7715e9e651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.6839, Accuracy: 0.8364\n","Epoch 2/10, Loss: 0.4466, Accuracy: 0.8364\n","Epoch 3/10, Loss: 0.4443, Accuracy: 0.8364\n","Epoch 4/10, Loss: 0.4446, Accuracy: 0.8364\n","Epoch 5/10, Loss: 0.4446, Accuracy: 0.8364\n","Epoch 6/10, Loss: 0.4432, Accuracy: 0.8364\n","Epoch 7/10, Loss: 0.4445, Accuracy: 0.8364\n","Epoch 8/10, Loss: 0.4447, Accuracy: 0.8364\n","Epoch 9/10, Loss: 0.4441, Accuracy: 0.8364\n","Epoch 10/10, Loss: 0.4430, Accuracy: 0.8364\n"]}]},{"cell_type":"code","source":["# 고차원 학습 시작\n","\n","# Custom Transformer 모델 수정\n","class CustomTransformerModel(nn.Module):\n","    def __init__(self, input_dim, num_heads, num_layers):\n","        super(CustomTransformerModel, self).__init__()\n","        self.input_layer = nn.Linear(input_dim, 64)  # 입력 차원을 64로 변환\n","        self.positional_encoding = PositionalEncoding(input_dim=64)  # Positional Encoding\n","        self.multihead_attn = nn.MultiheadAttention(embed_dim=64, num_heads=num_heads, batch_first=True)  # Multi-Head Attention\n","        self.encoder_layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=64, nhead=num_heads, batch_first=True, dropout=0.1)\n","            for _ in range(num_layers)\n","        ])\n","        self.feedforward1 = nn.Linear(64, 32)  # 첫 번째 Feedforward Layer\n","        self.activation = nn.ReLU()  # 활성화 함수\n","        self.feedforward2 = nn.Linear(32, 16)  # 두 번째 Feedforward Layer\n","        self.output_layer = nn.Linear(16, 1)  # 최종 출력 레이어\n","        self.attention_weights = None  # Attention Weights 저장 변수\n","\n","    def forward(self, x):\n","        x = self.input_layer(x).unsqueeze(1)  # [Batch, Sequence Length, Feature Dimension]\n","        x = self.positional_encoding(x)\n","\n","        # Multi-Head Attention\n","        attn_output, attn_weights = self.multihead_attn(x, x, x)\n","        self.attention_weights = attn_weights  # Attention Weights 저장\n","\n","        # Transformer Encoder Layers\n","        for layer in self.encoder_layers:\n","            x = layer(x)\n","\n","        # Feedforward Layers\n","        x = self.feedforward1(x[:, 0, :])  # 첫 번째 Feedforward\n","        x = self.activation(x)  # 활성화 함수\n","        x = self.feedforward2(x)  # 두 번째 Feedforward\n","        x = self.output_layer(x)  # 최종 출력 계산\n","        return torch.sigmoid(x)  # 이진 분류를 위한 Sigmoid"],"metadata":{"id":"5yoWp75JWsqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 초기화\n","model = CustomTransformerModel(input_dim=X_train.shape[1], num_heads=4, num_layers=2)\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# 학습 및 평가\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Forward Pass\n","    outputs = model(X_train).squeeze()\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","\n","    # 평가\n","    model.eval()\n","    with torch.no_grad():\n","        test_outputs = model(X_test).squeeze()\n","        predictions = (test_outputs >= 0.5).float()\n","        accuracy = (predictions == y_test).sum().item() / len(y_test)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcDdF8DCWvLr","executionInfo":{"status":"ok","timestamp":1735189531279,"user_tz":-540,"elapsed":1272,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"092d70ca-977d-42e6-fe79-8a29d35eebc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.7426, Accuracy: 0.8364\n","Epoch 2/10, Loss: 0.6393, Accuracy: 0.8364\n","Epoch 3/10, Loss: 0.5970, Accuracy: 0.8364\n","Epoch 4/10, Loss: 0.5713, Accuracy: 0.8364\n","Epoch 5/10, Loss: 0.5522, Accuracy: 0.8364\n","Epoch 6/10, Loss: 0.5361, Accuracy: 0.8364\n","Epoch 7/10, Loss: 0.5228, Accuracy: 0.8364\n","Epoch 8/10, Loss: 0.5113, Accuracy: 0.8364\n","Epoch 9/10, Loss: 0.5012, Accuracy: 0.8364\n","Epoch 10/10, Loss: 0.4905, Accuracy: 0.8364\n"]}]},{"cell_type":"code","source":["# Output Layer 모델 정의\n","\n","class CustomTransformerModel(nn.Module):\n","    def __init__(self, input_dim, num_heads, num_layers):\n","        super(CustomTransformerModel, self).__init__()\n","        self.input_layer = nn.Linear(input_dim, 64)  # 입력 차원을 64로 변환\n","        self.positional_encoding = PositionalEncoding(input_dim=64)  # Positional Encoding\n","        self.multihead_attn = nn.MultiheadAttention(embed_dim=64, num_heads=num_heads, batch_first=True)  # Multi-Head Attention\n","        self.encoder_layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=64, nhead=num_heads, batch_first=True, dropout=0.1)\n","            for _ in range(num_layers)\n","        ]의\n","        self.feedforward1 = nn.Linear(64, 32)  # 첫 번째 Feedforward Layer\n","        self.activation = nn.ReLU()  # 활성화 함수\n","        self.feedforward2 = nn.Linear(32, 16)  # 두 번째 Feedforward Layer\n","        self.output_layer = nn.Linear(16, 1)  # 최종 출력 레이어\n","        self.attention_weights = None  # Attention Weights 저장 변수\n","\n","    def forward(self, x):\n","        x = self.input_layer(x).unsqueeze(1)  # [Batch, Sequence Length, Feature Dimension]\n","        x = self.positional_encoding(x)\n","\n","        # Multi-Head Attention\n","        attn_output, attn_weights = self.multihead_attn(x, x, x)\n","        self.attention_weights = attn_weights  # Attention Weights 저장\n","\n","        # Transformer Encoder Layers\n","        for layer in self.encoder_layers:\n","            x = layer(x)\n","\n","        # Feedforward Layers\n","        x = self.feedforward1(x[:, 0, :])  # 첫 번째 Feedforward\n","        x = self.activation(x)  # 활성화 함수\n","        x = self.feedforward2(x)  # 두 번째 Feedforward\n","\n","        # Output Layer\n","        x = self.output_layer(x)  # 최종 출력 계산\n","        return torch.sigmoid(x)  # 이진 분류를 위한 Sigmoid"],"metadata":{"id":"MC9EKZmMW5ti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 초기화\n","model = CustomTransformerModel(input_dim=X_train.shape[1], num_heads=4, num_layers=2)\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# 학습 및 평가\n","epochs = 100\n","for epoch in range(epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Forward Pass\n","    outputs = model(X_train).squeeze()\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","\n","    # 평가\n","    model.eval()\n","    with torch.no_grad():\n","        test_outputs = model(X_test).squeeze()\n","        predictions = (test_outputs >= 0.5).float()\n","        accuracy = (predictions == y_test).sum().item() / len(y_test)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3aG6C44XFQk","executionInfo":{"status":"ok","timestamp":1735189864563,"user_tz":-540,"elapsed":12315,"user":{"displayName":"복제비Tv","userId":"00328494035627020412"}},"outputId":"e0a7db4e-3435-4f80-ace2-cca42c1b2182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 0.7896, Accuracy: 0.8364\n","Epoch 2/100, Loss: 0.5614, Accuracy: 0.8364\n","Epoch 3/100, Loss: 0.5087, Accuracy: 0.8364\n","Epoch 4/100, Loss: 0.4834, Accuracy: 0.8364\n","Epoch 5/100, Loss: 0.4692, Accuracy: 0.8364\n","Epoch 6/100, Loss: 0.4585, Accuracy: 0.8364\n","Epoch 7/100, Loss: 0.4514, Accuracy: 0.8364\n","Epoch 8/100, Loss: 0.4482, Accuracy: 0.8364\n","Epoch 9/100, Loss: 0.4470, Accuracy: 0.8364\n","Epoch 10/100, Loss: 0.4460, Accuracy: 0.8364\n","Epoch 11/100, Loss: 0.4449, Accuracy: 0.8364\n","Epoch 12/100, Loss: 0.4456, Accuracy: 0.8364\n","Epoch 13/100, Loss: 0.4449, Accuracy: 0.8364\n","Epoch 14/100, Loss: 0.4461, Accuracy: 0.8364\n","Epoch 15/100, Loss: 0.4472, Accuracy: 0.8364\n","Epoch 16/100, Loss: 0.4475, Accuracy: 0.8364\n","Epoch 17/100, Loss: 0.4468, Accuracy: 0.8364\n","Epoch 18/100, Loss: 0.4456, Accuracy: 0.8364\n","Epoch 19/100, Loss: 0.4458, Accuracy: 0.8364\n","Epoch 20/100, Loss: 0.4443, Accuracy: 0.8364\n","Epoch 21/100, Loss: 0.4467, Accuracy: 0.8364\n","Epoch 22/100, Loss: 0.4458, Accuracy: 0.8364\n","Epoch 23/100, Loss: 0.4440, Accuracy: 0.8364\n","Epoch 24/100, Loss: 0.4444, Accuracy: 0.8364\n","Epoch 25/100, Loss: 0.4452, Accuracy: 0.8364\n","Epoch 26/100, Loss: 0.4452, Accuracy: 0.8364\n","Epoch 27/100, Loss: 0.4456, Accuracy: 0.8364\n","Epoch 28/100, Loss: 0.4449, Accuracy: 0.8364\n","Epoch 29/100, Loss: 0.4440, Accuracy: 0.8364\n","Epoch 30/100, Loss: 0.4454, Accuracy: 0.8364\n","Epoch 31/100, Loss: 0.4437, Accuracy: 0.8364\n","Epoch 32/100, Loss: 0.4437, Accuracy: 0.8364\n","Epoch 33/100, Loss: 0.4420, Accuracy: 0.8364\n","Epoch 34/100, Loss: 0.4425, Accuracy: 0.8364\n","Epoch 35/100, Loss: 0.4433, Accuracy: 0.8364\n","Epoch 36/100, Loss: 0.4417, Accuracy: 0.8364\n","Epoch 37/100, Loss: 0.4421, Accuracy: 0.8364\n","Epoch 38/100, Loss: 0.4410, Accuracy: 0.8364\n","Epoch 39/100, Loss: 0.4420, Accuracy: 0.8364\n","Epoch 40/100, Loss: 0.4387, Accuracy: 0.8364\n","Epoch 41/100, Loss: 0.4373, Accuracy: 0.8364\n","Epoch 42/100, Loss: 0.4377, Accuracy: 0.8364\n","Epoch 43/100, Loss: 0.4337, Accuracy: 0.8364\n","Epoch 44/100, Loss: 0.4347, Accuracy: 0.8424\n","Epoch 45/100, Loss: 0.4332, Accuracy: 0.8485\n","Epoch 46/100, Loss: 0.4312, Accuracy: 0.8424\n","Epoch 47/100, Loss: 0.4349, Accuracy: 0.8424\n","Epoch 48/100, Loss: 0.4302, Accuracy: 0.8364\n","Epoch 49/100, Loss: 0.4340, Accuracy: 0.8424\n","Epoch 50/100, Loss: 0.4298, Accuracy: 0.8364\n","Epoch 51/100, Loss: 0.4297, Accuracy: 0.8364\n","Epoch 52/100, Loss: 0.4303, Accuracy: 0.8364\n","Epoch 53/100, Loss: 0.4356, Accuracy: 0.8364\n","Epoch 54/100, Loss: 0.4313, Accuracy: 0.8364\n","Epoch 55/100, Loss: 0.4286, Accuracy: 0.8364\n","Epoch 56/100, Loss: 0.4281, Accuracy: 0.8364\n","Epoch 57/100, Loss: 0.4278, Accuracy: 0.8364\n","Epoch 58/100, Loss: 0.4293, Accuracy: 0.8364\n","Epoch 59/100, Loss: 0.4259, Accuracy: 0.8364\n","Epoch 60/100, Loss: 0.4249, Accuracy: 0.8364\n","Epoch 61/100, Loss: 0.4278, Accuracy: 0.8364\n","Epoch 62/100, Loss: 0.4253, Accuracy: 0.8364\n","Epoch 63/100, Loss: 0.4278, Accuracy: 0.8364\n","Epoch 64/100, Loss: 0.4264, Accuracy: 0.8364\n","Epoch 65/100, Loss: 0.4275, Accuracy: 0.8364\n","Epoch 66/100, Loss: 0.4251, Accuracy: 0.8364\n","Epoch 67/100, Loss: 0.4215, Accuracy: 0.8364\n","Epoch 68/100, Loss: 0.4247, Accuracy: 0.8364\n","Epoch 69/100, Loss: 0.4239, Accuracy: 0.8364\n","Epoch 70/100, Loss: 0.4250, Accuracy: 0.8364\n","Epoch 71/100, Loss: 0.4223, Accuracy: 0.8364\n","Epoch 72/100, Loss: 0.4226, Accuracy: 0.8364\n","Epoch 73/100, Loss: 0.4225, Accuracy: 0.8364\n","Epoch 74/100, Loss: 0.4208, Accuracy: 0.8364\n","Epoch 75/100, Loss: 0.4206, Accuracy: 0.8364\n","Epoch 76/100, Loss: 0.4186, Accuracy: 0.8364\n","Epoch 77/100, Loss: 0.4217, Accuracy: 0.8364\n","Epoch 78/100, Loss: 0.4208, Accuracy: 0.8364\n","Epoch 79/100, Loss: 0.4205, Accuracy: 0.8364\n","Epoch 80/100, Loss: 0.4192, Accuracy: 0.8364\n","Epoch 81/100, Loss: 0.4202, Accuracy: 0.8364\n","Epoch 82/100, Loss: 0.4164, Accuracy: 0.8364\n","Epoch 83/100, Loss: 0.4175, Accuracy: 0.8364\n","Epoch 84/100, Loss: 0.4195, Accuracy: 0.8364\n","Epoch 85/100, Loss: 0.4215, Accuracy: 0.8364\n","Epoch 86/100, Loss: 0.4209, Accuracy: 0.8364\n","Epoch 87/100, Loss: 0.4180, Accuracy: 0.8364\n","Epoch 88/100, Loss: 0.4155, Accuracy: 0.8364\n","Epoch 89/100, Loss: 0.4272, Accuracy: 0.8364\n","Epoch 90/100, Loss: 0.4292, Accuracy: 0.8364\n","Epoch 91/100, Loss: 0.4314, Accuracy: 0.8364\n","Epoch 92/100, Loss: 0.4167, Accuracy: 0.8364\n","Epoch 93/100, Loss: 0.4214, Accuracy: 0.8364\n","Epoch 94/100, Loss: 0.4271, Accuracy: 0.8364\n","Epoch 95/100, Loss: 0.4226, Accuracy: 0.8364\n","Epoch 96/100, Loss: 0.4214, Accuracy: 0.8364\n","Epoch 97/100, Loss: 0.4221, Accuracy: 0.8364\n","Epoch 98/100, Loss: 0.4239, Accuracy: 0.8364\n","Epoch 99/100, Loss: 0.4205, Accuracy: 0.8364\n","Epoch 100/100, Loss: 0.4219, Accuracy: 0.8364\n"]}]}]}